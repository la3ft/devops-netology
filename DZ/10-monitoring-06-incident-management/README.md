# Домашнее задание к занятию "10.06. Инцидент-менеджмент"

## Задание 1

Составьте постмотрем, на основе реального сбоя системы Github в 2018 году.

Информация о сбое доступна [в виде краткой выжимки на русском языке](https://habr.com/ru/post/427301/) , а
также [развёрнуто на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).

## Ответ: 
| **Вид события**  | **Описание** |
| ------------- | ------------- |
| **Краткое описание инцидента (краткая выжимка о инциденте)** | В 22:52 UTC 21 октября 2018 в результате работ на 100G оптическом оборудовании произошли проблемы на сетевых разделах и последующих сбоях в БД. В результата возникла несогласованность данных на сервисах. Произошла деградация сервиса в течении 24 часов и 11 минут. |
| **Предшествующие события (что произошло перед инцидентом)** | Работы по техническому обслуживанию для замены неисправного оптического оборудования 100G привели к потере связи между сетевым центром Западного побережья США и основным центром обработки данных на Восточном побережье США. |
| **Причина инцидента (из-за чего возник инцидент)** | Выполнялись работы по техническому обслуживанию для замены неисправного оптического оборудования 100G привели к потере связи между сетевым центром Западного побережья США и основным центром обработки данных на Восточном побережье США. Связь между этими местоположениями была восстановлена за 43 секунды, но это короткое отключение вызвало цепочку событий, которые привели к 24-часовому и 11-минутному ухудшению обслуживания. |
| **Воздействие (на что повлиял инцидент)** | Пострадали несколько внутренних систем, что привело к отображению информации, которая была устаревшей и непоследовательной. В конечном счете, никакие пользовательские данные не были потеряны. На протяжении большей части инцидента GitHub также не мог обслуживать события веб-перехватчика или создавать и публиковать сайты GitHub Pages. |
| **Обнаружение (когда и как инцидент был обнаружен)** | Инженеры команды быстрого реагирования определили, что топологии для многочисленных кластеров баз данных находятся в неожиданном состоянии. |
| **Реакция (кто ответил на инцидент, кто был привлечен, какие каналы коммуникации были задействованы)** | Через 15 минут после начала, инцидент переведен в высший приоритет. Через 1 час 30 минут начато восстановление из резервных копий. Дальнейшие работы по восстановлению различных систем продолжались следующие 22 часа. После восстановления, работы продолжились по восстановлению консистентности данных. |
| **Восстановление (описание действий по устранению инцидента и поведение системы)** | Было выполнено восстановления данных из бекапов и репликации всех имеющихся данных с хостов с актуальными данными для восстановления 100% целостности данных во всех кластерах хранения данных. |
| **Таймлайн (последовательное описание ключевых событий инцидента с указанием времени)** | 2018.10.21 22:52 UTC - потеря консенсуса между серверами в дата-центрах в результате описанного инцидента. После восстановления была попытка восстановления целостности кластера, восстановления консенсуса, но данные в БД различались что привело к несогласованности в рамках кластера  
| | 2018.10.21 22:54 UTC - Мониторинг генерировал Алерты, инженеры поддержки реагировали на алерты, в 23:02 обнаружили несоответствие статуса кластера ожидаемому. Выявлено отсутсвие серверов из US East Coast  
| | 2018.10.21 23:07 UTC - отключены внутренние инструменны развертывания для предотвращения дополнительных имзменй. Сайт переведен в желтый статус и автомтически зафикисрован инциден в системе управления сбоями  
| | 2018.10.21 23:13 UTC - после выявления воздействия на множественные сервера, выведены дополнительные инженеры, выполнены действия для сохранения пользовательских данных, но деградация системы не была остановлена  
| | 2018.10.21 23:19 UTC - Были остановлены некоторые процессы (принудительная деградация системы) с целью повышения скорости восстановления  
| | 2018.10.22 00:05 UTC - Разработка плана по восстановления системы и синхронизации репликаций данных. Обновлен статус, чтобы сообщить пользователям, что мы собираемся выполнить контролируемую отработку отказа внутренней системы хранения данных.  
| | 2018.10.22 00:41 UTC - Запущен процесс бекапирования данных, мониторинг состояния работ  
| | 2018.10.22 06:51 UTC - бэкапы выполнены US East Coast data center и запущенно реплицирование с серверов в West Coast.  
| | 2018.10.22 07:46 UTC - опубликована расширенная информация для пользователей  
| | 2018.10.22 11:12 UTC - Восстановлены сервера в US East Coast, продолжается реплицирование. Наблюдается повышенная нагрузка при реплицировании.  
| | 2018.10.22 13:15 UTC - Приближались к пиковому периоду нагрузок. Увеличили количество репликаций для снятии растущей нагрузки по реплицированию.  
| | 2018.10.22 16:24 UTC - Реплицирование синхронизировано, переключение в штатную топологию MySQL  
| | 2018.10.22 16:45 UTC - После восстановления возникла необходимость балансировки нагрузки для восстановления 100% услуг клиентам. Для восстановления уже имеющихся данных пользователей включили обработку, так же подняли TTL до полного завершения и возвращения к штатной работе.  
| | 2018.10.22 23:03 UTC - Работа возвращена к штатному поведению |
| **Последующие действия (что нужно предпринять, чтобы инцидент не повторялся)** | *Краткосрочно:*
| | Собранны логи по всем серверам. Производится анализ логов для выявления запросов требующих восстановления консистентности.  
| | *Долгосрочно:*
| | Настройка конфигурациии Оркестратора чтобы исключить автоматическую миграцию кластеров между регионами
| | Ускорить работы над новым методом уведомления пользователе о состоянии сервиса
| | Поднять срочность работ по наращиванию избыточности ЦОД и программы по реструктуризации систем для работы в режиме активный-активный
| | Прилагать больше усилий на проверку предположений о работе и производительности сервсов
| | Начать программу проверки сценариев отказа, включая методы chaos engineering |
